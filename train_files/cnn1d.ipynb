{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b155fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from models import CNN1d\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5dea10",
   "metadata": {},
   "source": [
    "## Set Seed to Ensure Reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5846d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 2023\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed):\n",
    "    # Set random seed for CPU\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set random seed for GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "seed = 2023  # You can replace this with your desired seed value\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fec5e8",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "<!-- * Training\n",
    "    * sEMG subjects: 1-24\n",
    "    * sEMG channels: 1, 2, 3, 4\n",
    "    * ECG subjects: [16265, 16272, 16273, 16420, 16483, 16539, 16773, 16786, 16795, 17052, 17453, 18177] (9F+3M)\n",
    "    * ECG channel: 1\n",
    "    * SNR: [0, -1, -2, ..., -14, -15]\n",
    "* Validation\n",
    "    * sEMG subjects: 25-32\n",
    "    * sEMG channels: 5, 6\n",
    "    * ECG subjects: [18184, 19088, 19090, 19093, 19140, 19830] (4F+2M)\n",
    "    * ECG channel: 1\n",
    "    * SNR: [0, -0.5, -1, ..., -14.5, -15]\n",
    "* Testing\n",
    "    * sEMG subjects: 33-40\n",
    "    * sEMG channels: 9, 10\n",
    "    * ECG subjects: [18184, 19088, 19090, 19093, 19140, 19830] (4F+2M)\n",
    "    * ECG channel: 2\n",
    "    * SNR: [0, -0.5, -1, ..., -14.5, -15] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eadff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (137792, 1, 10000)\n",
      "Validation data shape: (43710, 1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Paths to annotation CSV files\n",
    "train_csv_path = \"train_annotations_E1.csv\"\n",
    "val_csv_path = \"val_annotations_E1.csv\"\n",
    "\n",
    "# Folder containing .npy files\n",
    "data_folder = \"./mixed_signals_E1\"\n",
    "\n",
    "# Read the CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "\n",
    "# Initialize lists to store data and labels\n",
    "train_data = []\n",
    "train_labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "# Read .npy files and their corresponding SNRs\n",
    "for _, row in train_df.iterrows():\n",
    "    file_name = row['mixed_name']\n",
    "    snr = np.array([row['snr']])\n",
    "    npy_file_path = os.path.join(data_folder, file_name)\n",
    "    data = np.load(npy_file_path)\n",
    "    data = data.reshape(1, 10000)\n",
    "    train_data.append(data)\n",
    "    train_labels.append(snr)\n",
    "    \n",
    "for _, row in val_df.iterrows():\n",
    "    file_name = row['mixed_name']\n",
    "    snr = np.array([row['snr']])\n",
    "    npy_file_path = os.path.join(data_folder, file_name)\n",
    "    data = np.load(npy_file_path)\n",
    "    data = data.reshape(1, 10000)\n",
    "    val_data.append(data)\n",
    "    val_labels.append(snr)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e4a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:5'\n",
    "train_data = torch.Tensor(train_data).to(device)\n",
    "train_labels = torch.Tensor(train_labels).to(device)\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = torch.Tensor(val_data).to(device)\n",
    "val_labels = torch.Tensor(val_labels).to(device)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d3f6f",
   "metadata": {},
   "source": [
    "## Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e46de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1d(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(16,), stride=(8,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(8,), stride=(4,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Number of trainable parameters:  37969\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = CNN1d()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "model_name = model.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88aae89",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1000/4306], Loss: 3.4739\n",
      "Epoch [1/30], Step [2000/4306], Loss: 4.3940\n",
      "Epoch [1/30], Step [3000/4306], Loss: 3.0835\n",
      "Epoch [1/30], Step [4000/4306], Loss: 4.9346\n",
      "Epoch [1/30], Mean Epoch Loss: 7.1518, Mean Validation Loss: 3.1989\n",
      "==================================================\n",
      "Epoch [2/30], Step [1000/4306], Loss: 3.1232\n",
      "Epoch [2/30], Step [2000/4306], Loss: 2.1543\n",
      "Epoch [2/30], Step [3000/4306], Loss: 1.2101\n",
      "Epoch [2/30], Step [4000/4306], Loss: 0.5259\n",
      "Epoch [2/30], Mean Epoch Loss: 1.7634, Mean Validation Loss: 1.9288\n",
      "==================================================\n",
      "Epoch [3/30], Step [1000/4306], Loss: 1.1463\n",
      "Epoch [3/30], Step [2000/4306], Loss: 2.0722\n",
      "Epoch [3/30], Step [3000/4306], Loss: 0.7162\n",
      "Epoch [3/30], Step [4000/4306], Loss: 3.0967\n",
      "Epoch [3/30], Mean Epoch Loss: 0.9321, Mean Validation Loss: 1.6028\n",
      "==================================================\n",
      "Epoch [4/30], Step [1000/4306], Loss: 0.5444\n",
      "Epoch [4/30], Step [2000/4306], Loss: 0.7064\n",
      "Epoch [4/30], Step [3000/4306], Loss: 0.4350\n",
      "Epoch [4/30], Step [4000/4306], Loss: 0.2771\n",
      "Epoch [4/30], Mean Epoch Loss: 0.5487, Mean Validation Loss: 1.3775\n",
      "==================================================\n",
      "Epoch [5/30], Step [1000/4306], Loss: 0.3196\n",
      "Epoch [5/30], Step [2000/4306], Loss: 0.4253\n",
      "Epoch [5/30], Step [3000/4306], Loss: 0.3878\n",
      "Epoch [5/30], Step [4000/4306], Loss: 0.2578\n",
      "Epoch [5/30], Mean Epoch Loss: 0.3792, Mean Validation Loss: 0.9868\n",
      "==================================================\n",
      "Epoch [6/30], Step [1000/4306], Loss: 0.3804\n",
      "Epoch [6/30], Step [2000/4306], Loss: 0.2190\n",
      "Epoch [6/30], Step [3000/4306], Loss: 0.2761\n",
      "Epoch [6/30], Step [4000/4306], Loss: 0.2735\n",
      "Epoch [6/30], Mean Epoch Loss: 0.2922, Mean Validation Loss: 0.8525\n",
      "==================================================\n",
      "Epoch [7/30], Step [1000/4306], Loss: 0.1655\n",
      "Epoch [7/30], Step [2000/4306], Loss: 0.2371\n",
      "Epoch [7/30], Step [3000/4306], Loss: 0.1467\n",
      "Epoch [7/30], Step [4000/4306], Loss: 0.3627\n",
      "Epoch [7/30], Mean Epoch Loss: 0.2425, Mean Validation Loss: 0.6799\n",
      "==================================================\n",
      "Epoch [8/30], Step [1000/4306], Loss: 0.1248\n",
      "Epoch [8/30], Step [2000/4306], Loss: 0.2315\n",
      "Epoch [8/30], Step [3000/4306], Loss: 0.1228\n",
      "Epoch [8/30], Step [4000/4306], Loss: 0.2492\n",
      "Epoch [8/30], Mean Epoch Loss: 0.2169, Mean Validation Loss: 0.6267\n",
      "==================================================\n",
      "Epoch [9/30], Step [1000/4306], Loss: 0.0991\n",
      "Epoch [9/30], Step [2000/4306], Loss: 0.1492\n",
      "Epoch [9/30], Step [3000/4306], Loss: 0.1804\n",
      "Epoch [9/30], Step [4000/4306], Loss: 0.3080\n",
      "Epoch [9/30], Mean Epoch Loss: 0.1968, Mean Validation Loss: 0.5061\n",
      "==================================================\n",
      "Epoch [10/30], Step [1000/4306], Loss: 0.1491\n",
      "Epoch [10/30], Step [2000/4306], Loss: 0.1640\n",
      "Epoch [10/30], Step [3000/4306], Loss: 0.2722\n",
      "Epoch [10/30], Step [4000/4306], Loss: 0.1050\n",
      "Epoch [10/30], Mean Epoch Loss: 0.1808, Mean Validation Loss: 0.5021\n",
      "==================================================\n",
      "Epoch [11/30], Step [1000/4306], Loss: 0.1714\n",
      "Epoch [11/30], Step [2000/4306], Loss: 0.2066\n",
      "Epoch [11/30], Step [3000/4306], Loss: 0.1206\n",
      "Epoch [11/30], Step [4000/4306], Loss: 0.1630\n",
      "Epoch [11/30], Mean Epoch Loss: 0.1686, Mean Validation Loss: 0.5262\n",
      "==================================================\n",
      "Epoch [12/30], Step [1000/4306], Loss: 0.2486\n",
      "Epoch [12/30], Step [2000/4306], Loss: 0.1390\n",
      "Epoch [12/30], Step [3000/4306], Loss: 0.0878\n",
      "Epoch [12/30], Step [4000/4306], Loss: 0.0840\n",
      "Epoch [12/30], Mean Epoch Loss: 0.1587, Mean Validation Loss: 0.4474\n",
      "==================================================\n",
      "Epoch [13/30], Step [1000/4306], Loss: 0.3974\n",
      "Epoch [13/30], Step [2000/4306], Loss: 0.1226\n",
      "Epoch [13/30], Step [3000/4306], Loss: 0.1748\n",
      "Epoch [13/30], Step [4000/4306], Loss: 0.1067\n",
      "Epoch [13/30], Mean Epoch Loss: 0.1530, Mean Validation Loss: 0.4216\n",
      "==================================================\n",
      "Epoch [14/30], Step [1000/4306], Loss: 0.0751\n",
      "Epoch [14/30], Step [2000/4306], Loss: 0.0887\n",
      "Epoch [14/30], Step [3000/4306], Loss: 0.1336\n",
      "Epoch [14/30], Step [4000/4306], Loss: 0.0724\n",
      "Epoch [14/30], Mean Epoch Loss: 0.1416, Mean Validation Loss: 0.5619\n",
      "==================================================\n",
      "Epoch [15/30], Step [1000/4306], Loss: 0.0765\n",
      "Epoch [15/30], Step [2000/4306], Loss: 0.1934\n",
      "Epoch [15/30], Step [3000/4306], Loss: 0.2219\n",
      "Epoch [15/30], Step [4000/4306], Loss: 0.1294\n",
      "Epoch [15/30], Mean Epoch Loss: 0.1395, Mean Validation Loss: 0.3752\n",
      "==================================================\n",
      "Epoch [16/30], Step [1000/4306], Loss: 0.1387\n",
      "Epoch [16/30], Step [2000/4306], Loss: 0.0949\n",
      "Epoch [16/30], Step [3000/4306], Loss: 0.1286\n",
      "Epoch [16/30], Step [4000/4306], Loss: 0.1239\n",
      "Epoch [16/30], Mean Epoch Loss: 0.1316, Mean Validation Loss: 0.3959\n",
      "==================================================\n",
      "Epoch [17/30], Step [1000/4306], Loss: 0.0754\n",
      "Epoch [17/30], Step [2000/4306], Loss: 0.1342\n",
      "Epoch [17/30], Step [3000/4306], Loss: 0.0975\n",
      "Epoch [17/30], Step [4000/4306], Loss: 0.1222\n",
      "Epoch [17/30], Mean Epoch Loss: 0.1286, Mean Validation Loss: 0.3891\n",
      "==================================================\n",
      "Epoch [18/30], Step [1000/4306], Loss: 0.1391\n",
      "Epoch [18/30], Step [2000/4306], Loss: 0.1113\n",
      "Epoch [18/30], Step [3000/4306], Loss: 0.0766\n",
      "Epoch [18/30], Step [4000/4306], Loss: 0.1297\n",
      "Epoch [18/30], Mean Epoch Loss: 0.1223, Mean Validation Loss: 0.3620\n",
      "==================================================\n",
      "Epoch [19/30], Step [1000/4306], Loss: 0.1946\n",
      "Epoch [19/30], Step [2000/4306], Loss: 0.1738\n",
      "Epoch [19/30], Step [3000/4306], Loss: 0.2095\n",
      "Epoch [19/30], Step [4000/4306], Loss: 0.1096\n",
      "Epoch [19/30], Mean Epoch Loss: 0.1185, Mean Validation Loss: 0.3659\n",
      "==================================================\n",
      "Epoch [20/30], Step [1000/4306], Loss: 0.0831\n",
      "Epoch [20/30], Step [2000/4306], Loss: 0.0875\n",
      "Epoch [20/30], Step [3000/4306], Loss: 0.0809\n",
      "Epoch [20/30], Step [4000/4306], Loss: 0.1065\n",
      "Epoch [20/30], Mean Epoch Loss: 0.1139, Mean Validation Loss: 0.3533\n",
      "==================================================\n",
      "Epoch [21/30], Step [1000/4306], Loss: 0.1037\n",
      "Epoch [21/30], Step [2000/4306], Loss: 0.1001\n",
      "Epoch [21/30], Step [3000/4306], Loss: 0.0909\n",
      "Epoch [21/30], Step [4000/4306], Loss: 0.0496\n",
      "Epoch [21/30], Mean Epoch Loss: 0.1098, Mean Validation Loss: 0.3150\n",
      "==================================================\n",
      "Epoch [22/30], Step [1000/4306], Loss: 0.1258\n",
      "Epoch [22/30], Step [2000/4306], Loss: 0.0791\n",
      "Epoch [22/30], Step [3000/4306], Loss: 0.0798\n",
      "Epoch [22/30], Step [4000/4306], Loss: 0.1457\n",
      "Epoch [22/30], Mean Epoch Loss: 0.1077, Mean Validation Loss: 0.3284\n",
      "==================================================\n",
      "Epoch [23/30], Step [1000/4306], Loss: 0.0522\n",
      "Epoch [23/30], Step [2000/4306], Loss: 0.0553\n",
      "Epoch [23/30], Step [3000/4306], Loss: 0.1030\n",
      "Epoch [23/30], Step [4000/4306], Loss: 0.0786\n",
      "Epoch [23/30], Mean Epoch Loss: 0.1040, Mean Validation Loss: 0.3183\n",
      "==================================================\n",
      "Epoch [24/30], Step [1000/4306], Loss: 0.1333\n",
      "Epoch [24/30], Step [2000/4306], Loss: 0.0647\n",
      "Epoch [24/30], Step [3000/4306], Loss: 0.0720\n",
      "Epoch [24/30], Step [4000/4306], Loss: 0.0979\n",
      "Epoch [24/30], Mean Epoch Loss: 0.1013, Mean Validation Loss: 0.3452\n",
      "==================================================\n",
      "Epoch [25/30], Step [1000/4306], Loss: 0.1231\n",
      "Epoch [25/30], Step [2000/4306], Loss: 0.0911\n",
      "Epoch [25/30], Step [3000/4306], Loss: 0.0759\n",
      "Epoch [25/30], Step [4000/4306], Loss: 0.2187\n",
      "Epoch [25/30], Mean Epoch Loss: 0.0997, Mean Validation Loss: 0.3153\n",
      "==================================================\n",
      "Epoch [26/30], Step [1000/4306], Loss: 0.0641\n",
      "Epoch [26/30], Step [2000/4306], Loss: 0.0722\n",
      "Epoch [26/30], Step [3000/4306], Loss: 0.0430\n",
      "Epoch [26/30], Step [4000/4306], Loss: 0.0561\n",
      "Epoch [26/30], Mean Epoch Loss: 0.0970, Mean Validation Loss: 0.4184\n",
      "==================================================\n",
      "Epoch [27/30], Step [1000/4306], Loss: 0.0736\n",
      "Epoch [27/30], Step [2000/4306], Loss: 0.0708\n",
      "Epoch [27/30], Step [3000/4306], Loss: 0.1124\n",
      "Epoch [27/30], Step [4000/4306], Loss: 0.0948\n",
      "Epoch [27/30], Mean Epoch Loss: 0.0947, Mean Validation Loss: 0.4378\n",
      "==================================================\n",
      "Epoch [28/30], Step [1000/4306], Loss: 0.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Step [2000/4306], Loss: 0.0675\n",
      "Epoch [28/30], Step [3000/4306], Loss: 0.0767\n",
      "Epoch [28/30], Step [4000/4306], Loss: 0.1101\n",
      "Epoch [28/30], Mean Epoch Loss: 0.0917, Mean Validation Loss: 0.3194\n",
      "==================================================\n",
      "Epoch [29/30], Step [1000/4306], Loss: 0.1027\n",
      "Epoch [29/30], Step [2000/4306], Loss: 0.0577\n",
      "Epoch [29/30], Step [3000/4306], Loss: 0.0565\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "steps_per_print = 1000  # Print loss every 1500 steps\n",
    "\n",
    "# Initialize a list to store the losses\n",
    "train_losses = []\n",
    "val_losses = []  # Initialize a list to store validation losses\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    step_counter = 0  # Initialize step counter for the epoch\n",
    "    epoch_loss = 0.0  # Initialize the epoch loss\n",
    "\n",
    "    for batch_data, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_data)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate the loss for the epoch\n",
    "\n",
    "        # Increment step counter\n",
    "        step_counter += 1\n",
    "\n",
    "        # Print loss every `steps_per_print` steps\n",
    "        if step_counter % steps_per_print == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step_counter}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and record the mean loss for the epoch\n",
    "    mean_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(mean_epoch_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data, val_labels in val_dataloader:\n",
    "            val_predictions = model(val_data)\n",
    "            val_loss += criterion(val_predictions, val_labels).item()\n",
    "\n",
    "    mean_val_loss = val_loss / len(val_dataloader)\n",
    "    val_losses.append(mean_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Epoch Loss: {mean_epoch_loss:.4f}, Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "    print(\"==================================================\")\n",
    "\n",
    "\n",
    "print(\"Training completed\")\n",
    "torch.save(model.state_dict(), f'./checkpoints/{model_name.lower()}_{num_epochs}eps_seed{seed}.pth')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training time in seconds\n",
    "training_time_seconds = end - start\n",
    "# Convert training time to minutes\n",
    "training_time_minutes = training_time_seconds // 60\n",
    "\n",
    "# Print the training time in minutes\n",
    "print(f\"Training time: {training_time_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()  # Show legend indicating the lines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242186f",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and labels\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "# Iterate through validation data\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in val_dataloader:\n",
    "        predictions = model(batch_data)\n",
    "        val_predictions.append(predictions.cpu())\n",
    "        val_labels.append(batch_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and labels\n",
    "val_predictions_np = np.concatenate(val_predictions, axis=0)\n",
    "val_labels_np = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "# Reshape predictions and labels\n",
    "val_predictions_flat = val_predictions_np.reshape(-1)\n",
    "val_labels_flat = val_labels_np.reshape(-1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(val_labels_flat, val_predictions_flat)\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "correlation_matrix = np.corrcoef(val_labels_flat, val_predictions_flat)\n",
    "correlation_coefficient = correlation_matrix[0, 1]\n",
    "spearmanr_cc, _ = stats.spearmanr(val_labels_flat, val_predictions_flat)\n",
    "\n",
    "\n",
    "# Plot predictions against true labels\n",
    "plt.scatter(val_labels_flat, val_predictions_flat, alpha=0.05, s=1, color='black')\n",
    "plt.xlabel('True SNR')\n",
    "plt.ylabel('Estimated SNR')\n",
    "plt.title(f'CC = {correlation_coefficient:.4f}')\n",
    "\n",
    "# Set x-axis and y-axis limits\n",
    "plt.xlim(-17.5, 2.5)\n",
    "plt.ylim(-17.5, 2.5)\n",
    "\n",
    "# Perform linear regression\n",
    "m, b = np.polyfit(val_labels_flat, val_predictions_flat, 1)\n",
    "plt.plot(val_labels_flat, m * val_labels_flat + b, label=f\"y = {m:.2f}x + {b:.2f}\")\n",
    "plt.plot(val_labels_flat, val_labels_flat, label=\"y = x\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(f'Correlation coefficient (CC): {correlation_coefficient:.4f}')\n",
    "print(f'Spearman rank-order correlation coefficient (SRCC): {spearmanr_cc:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526eeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the square error for each element\n",
    "error = (val_labels_flat - val_predictions_flat)**2\n",
    "\n",
    "# Calculate mean and standard deviation of the error\n",
    "mean_error = np.mean(error)\n",
    "std_deviation_error = np.std(error)\n",
    "\n",
    "# Plot the error distribution as a histogram\n",
    "hist, bins, _ = plt.hist(error, bins=10, alpha=0.7, color='blue', edgecolor='black', range=(0, 5))\n",
    "plt.xlabel('Squared Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Squared Error Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate each bar with the frequency\n",
    "for i in range(len(hist)):\n",
    "    plt.text(bins[i] + (bins[i+1] - bins[i]) / 2, hist[i], f'{int(hist[i])}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Error:\", mean_error)\n",
    "print(\"Standard Deviation of Error:\", std_deviation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute error for each element\n",
    "error = np.abs(val_labels_flat - val_predictions_flat)\n",
    "\n",
    "# Calculate mean and standard deviation of the error\n",
    "mean_error = np.mean(error)\n",
    "std_deviation_error = np.std(error)\n",
    "\n",
    "# Plot the error distribution as a histogram\n",
    "hist, bins, _ = plt.hist(error, bins=10, alpha=0.7, color='blue', edgecolor='black', range=(0, 5))\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Squared Error')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate each bar with the frequency\n",
    "for i in range(len(hist)):\n",
    "    plt.text(bins[i] + (bins[i+1] - bins[i]) / 2, hist[i], f'{int(hist[i])}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mean_error)\n",
    "print(\"Standard Deviation of Absolute Error:\", std_deviation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615f6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snr",
   "language": "python",
   "name": "snr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
