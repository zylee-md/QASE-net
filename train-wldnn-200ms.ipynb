{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b155fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from models import WLDNN\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5dea10",
   "metadata": {},
   "source": [
    "## Set Seed to Ensure Reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5846d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 2023\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed):\n",
    "    # Set random seed for CPU\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set random seed for GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "seed = 2023  # You can replace this with your desired seed value\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fec5e8",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "<!-- * Training\n",
    "    * sEMG subjects: 1-24\n",
    "    * sEMG channels: 1, 2\n",
    "    * ECG subjects: [16265, 16272, 16273, 16420, 16483, 16539, 16773, 16786, 16795, 17052, 17453, 18177] (9F+3M)\n",
    "    * ECG channel: 1\n",
    "    * SNR: [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
    "* Validation\n",
    "    * sEMG subjects: 25-32\n",
    "    * sEMG channels: 5, 6\n",
    "    * ECG subjects: [18184, 19088, 19090, 19093, 19140, 19830] (4F+2M)\n",
    "    * ECG channel: 1\n",
    "    * SNR: [0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, -4, -4.5, -5, -5.5, -6, -6.5, -7, -7.5, -8, -8.5, -9, -9.5, -10]\n",
    "* Testing\n",
    "    * sEMG subjects: 33-40\n",
    "    * sEMG channels: 9, 10\n",
    "    * ECG subjects: [18184, 19088, 19090, 19093, 19140, 19830] (4F+2M)\n",
    "    * ECG channel: 2\n",
    "    * SNR: [0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, -4, -4.5, -5, -5.5, -6, -6.5, -7, -7.5, -8, -8.5, -9, -9.5, -10] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eaab907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 200\n",
    "\n",
    "def calculate_waveform_length_segment(segment):\n",
    "    # Calculate the waveform length as the sum of the absolute differences between adjacent samples in the segment.\n",
    "    return np.sum(np.abs(np.diff(segment)))\n",
    "\n",
    "def calculate_waveform_length(signal, window_size=WINDOW_SIZE, overlap=0):\n",
    "    # Calculate the number of segments based on segment size and overlap.\n",
    "    signal_length = len(signal)\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    num_segments = (signal_length - window_size) // step_size + 1\n",
    "\n",
    "    segment_waveform_lengths = []\n",
    "    for i in range(num_segments):\n",
    "        start = i * step_size\n",
    "        end = start + window_size\n",
    "        segment = signal[start:end]\n",
    "        \n",
    "        # Calculate the waveform length for the current segment and store it in the result array.\n",
    "        segment_length = calculate_waveform_length_segment(segment)\n",
    "        segment_waveform_lengths.append(segment_length)\n",
    "    \n",
    "    return np.array(segment_waveform_lengths)\n",
    "\n",
    "def mean_variance_normalize(time_series):\n",
    "    # Calculate the mean and standard deviation of the time series.\n",
    "    mean = np.mean(time_series)\n",
    "    std_dev = np.std(time_series)\n",
    "    \n",
    "    # Ensure there's no division by zero.\n",
    "    if std_dev == 0:\n",
    "        raise ValueError(\"Cannot normalize: Standard deviation is zero.\")\n",
    "    \n",
    "    # Normalize the time series to have mean 0 and standard deviation 1.\n",
    "    normalized_series = (time_series - mean) / std_dev\n",
    "    \n",
    "    return normalized_series\n",
    "\n",
    "def normalize_to_unit_energy(signal):\n",
    "    # Calculate the energy of the signal as the sum of the squares of its samples.\n",
    "    energy = np.sum(np.abs(signal)**2)\n",
    "    \n",
    "    # Ensure there's no division by zero.\n",
    "    if energy == 0:\n",
    "        raise ValueError(\"Cannot normalize: Energy is zero.\")\n",
    "    \n",
    "    # Normalize the signal to have unit energy by dividing by the square root of the energy.\n",
    "    normalized_signal = signal / np.sqrt(energy)\n",
    "    \n",
    "    return normalized_signal\n",
    "\n",
    "def calculate_overall_mean_std(array):\n",
    "    # Calculate the mean and standard deviation of the input array.\n",
    "    overall_mean = np.mean(array)\n",
    "    overall_std = np.std(array)\n",
    "    \n",
    "    # Print the calculated values for reference.\n",
    "    print(f\"train_mean = {overall_mean}\")\n",
    "    print(f\"train_std = {overall_std}\")\n",
    "    \n",
    "    return overall_mean, overall_std\n",
    "\n",
    "def normalize_dataset(data, mean, std):\n",
    "    # Normalize the dataset by subtracting the mean and dividing by the standard deviation.\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "# Example usage:\n",
    "test_sig = np.random.rand(10000,)\n",
    "print(test_sig.shape)\n",
    "test_wl = calculate_waveform_length(test_sig)\n",
    "print(test_wl.shape)\n",
    "input_length = test_wl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eadff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mean = 0.4347099520974417\n",
      "train_std = 0.3951968331310125\n",
      "Train data shape: (137792, 50)\n",
      "Validation data shape: (43710, 50)\n"
     ]
    }
   ],
   "source": [
    "# Paths to annotation CSV files\n",
    "train_csv_path = \"train_annotations_E1.csv\"\n",
    "val_csv_path = \"val_annotations_E1.csv\"\n",
    "\n",
    "# Folder containing .npy files\n",
    "data_folder = \"./mixed_signals_E1\"\n",
    "\n",
    "# Read the CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "\n",
    "# Initialize lists to store data and labels\n",
    "train_data = []\n",
    "train_labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "# Read .npy files and their corresponding SNRs\n",
    "for _, row in train_df.iterrows():\n",
    "    file_name = row['mixed_name']\n",
    "    snr = np.array([row['snr']])\n",
    "    npy_file_path = os.path.join(data_folder, file_name)\n",
    "    data = np.load(npy_file_path)\n",
    "    data = mean_variance_normalize(data)\n",
    "    data = normalize_to_unit_energy(data)\n",
    "    data = calculate_waveform_length(data)\n",
    "    train_data.append(data)\n",
    "    train_labels.append(snr)\n",
    "    \n",
    "for _, row in val_df.iterrows():\n",
    "    file_name = row['mixed_name']\n",
    "    snr = np.array([row['snr']])\n",
    "    npy_file_path = os.path.join(data_folder, file_name)\n",
    "    data = np.load(npy_file_path)\n",
    "    data = mean_variance_normalize(data)\n",
    "    data = normalize_to_unit_energy(data)\n",
    "    data = calculate_waveform_length(data)\n",
    "    val_data.append(data)\n",
    "    val_labels.append(snr)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Normalize data\n",
    "train_mean, train_std = calculate_overall_mean_std(train_data)\n",
    "normalize_dataset(train_data, train_mean, train_std)\n",
    "normalize_dataset(val_data, train_mean, train_std)\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e4a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:6'\n",
    "train_data = torch.Tensor(train_data).to(device)\n",
    "train_labels = torch.Tensor(train_labels).to(device)\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = torch.Tensor(val_data).to(device)\n",
    "val_labels = torch.Tensor(val_labels).to(device)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d3f6f",
   "metadata": {},
   "source": [
    "## 3. Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e46de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLDNN(\n",
      "  (hidden1): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (hidden2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (output): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Number of trainable parameters:  1461\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = WLDNN(input_length)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "model_name = model.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88aae89",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1000/4306], Loss: 18.3796\n",
      "Epoch [1/100], Step [2000/4306], Loss: 20.7903\n",
      "Epoch [1/100], Step [3000/4306], Loss: 28.8728\n",
      "Epoch [1/100], Step [4000/4306], Loss: 24.1759\n",
      "Epoch [1/100], Mean Epoch Loss: 22.8662, Mean Validation Loss: 20.0790\n",
      "==================================================\n",
      "Epoch [2/100], Step [1000/4306], Loss: 17.3417\n",
      "Epoch [2/100], Step [2000/4306], Loss: 21.8843\n",
      "Epoch [2/100], Step [3000/4306], Loss: 21.2093\n",
      "Epoch [2/100], Step [4000/4306], Loss: 25.1089\n",
      "Epoch [2/100], Mean Epoch Loss: 21.2593, Mean Validation Loss: 20.0548\n",
      "==================================================\n",
      "Epoch [3/100], Step [1000/4306], Loss: 23.0457\n",
      "Epoch [3/100], Step [2000/4306], Loss: 23.6928\n",
      "Epoch [3/100], Step [3000/4306], Loss: 23.1222\n",
      "Epoch [3/100], Step [4000/4306], Loss: 16.9810\n",
      "Epoch [3/100], Mean Epoch Loss: 21.2585, Mean Validation Loss: 20.0545\n",
      "==================================================\n",
      "Epoch [4/100], Step [1000/4306], Loss: 10.2155\n",
      "Epoch [4/100], Step [2000/4306], Loss: 6.3043\n",
      "Epoch [4/100], Step [3000/4306], Loss: 3.8619\n",
      "Epoch [4/100], Step [4000/4306], Loss: 6.8168\n",
      "Epoch [4/100], Mean Epoch Loss: 9.2250, Mean Validation Loss: 6.3206\n",
      "==================================================\n",
      "Epoch [5/100], Step [1000/4306], Loss: 9.2101\n",
      "Epoch [5/100], Step [2000/4306], Loss: 4.8440\n",
      "Epoch [5/100], Step [3000/4306], Loss: 6.0499\n",
      "Epoch [5/100], Step [4000/4306], Loss: 2.1549\n",
      "Epoch [5/100], Mean Epoch Loss: 4.9139, Mean Validation Loss: 7.1393\n",
      "==================================================\n",
      "Epoch [6/100], Step [1000/4306], Loss: 6.7576\n",
      "Epoch [6/100], Step [2000/4306], Loss: 6.0331\n",
      "Epoch [6/100], Step [3000/4306], Loss: 5.7171\n",
      "Epoch [6/100], Step [4000/4306], Loss: 7.7417\n",
      "Epoch [6/100], Mean Epoch Loss: 4.7676, Mean Validation Loss: 6.3068\n",
      "==================================================\n",
      "Epoch [7/100], Step [1000/4306], Loss: 6.6951\n",
      "Epoch [7/100], Step [2000/4306], Loss: 2.9131\n",
      "Epoch [7/100], Step [3000/4306], Loss: 4.7623\n",
      "Epoch [7/100], Step [4000/4306], Loss: 4.3649\n",
      "Epoch [7/100], Mean Epoch Loss: 4.5096, Mean Validation Loss: 3.1233\n",
      "==================================================\n",
      "Epoch [8/100], Step [1000/4306], Loss: 2.9067\n",
      "Epoch [8/100], Step [2000/4306], Loss: 2.2709\n",
      "Epoch [8/100], Step [3000/4306], Loss: 1.9959\n",
      "Epoch [8/100], Step [4000/4306], Loss: 3.5135\n",
      "Epoch [8/100], Mean Epoch Loss: 3.3474, Mean Validation Loss: 2.7854\n",
      "==================================================\n",
      "Epoch [9/100], Step [1000/4306], Loss: 3.1245\n",
      "Epoch [9/100], Step [2000/4306], Loss: 3.0005\n",
      "Epoch [9/100], Step [3000/4306], Loss: 4.0719\n",
      "Epoch [9/100], Step [4000/4306], Loss: 5.3039\n",
      "Epoch [9/100], Mean Epoch Loss: 3.1756, Mean Validation Loss: 2.3613\n",
      "==================================================\n",
      "Epoch [10/100], Step [1000/4306], Loss: 3.2686\n",
      "Epoch [10/100], Step [2000/4306], Loss: 3.8354\n",
      "Epoch [10/100], Step [3000/4306], Loss: 1.9143\n",
      "Epoch [10/100], Step [4000/4306], Loss: 2.7548\n",
      "Epoch [10/100], Mean Epoch Loss: 2.9996, Mean Validation Loss: 2.3357\n",
      "==================================================\n",
      "Model saved at epoch 10: ./checkpoints/wldnn_window200_10eps_seed2023.pth\n",
      "Epoch [11/100], Step [1000/4306], Loss: 3.1763\n",
      "Epoch [11/100], Step [2000/4306], Loss: 1.7649\n",
      "Epoch [11/100], Step [3000/4306], Loss: 3.1171\n",
      "Epoch [11/100], Step [4000/4306], Loss: 3.0815\n",
      "Epoch [11/100], Mean Epoch Loss: 2.9337, Mean Validation Loss: 2.5057\n",
      "==================================================\n",
      "Epoch [12/100], Step [1000/4306], Loss: 4.2203\n",
      "Epoch [12/100], Step [2000/4306], Loss: 2.5603\n",
      "Epoch [12/100], Step [3000/4306], Loss: 1.7915\n",
      "Epoch [12/100], Step [4000/4306], Loss: 2.7450\n",
      "Epoch [12/100], Mean Epoch Loss: 2.8830, Mean Validation Loss: 2.2835\n",
      "==================================================\n",
      "Epoch [13/100], Step [1000/4306], Loss: 2.9319\n",
      "Epoch [13/100], Step [2000/4306], Loss: 2.6293\n",
      "Epoch [13/100], Step [3000/4306], Loss: 3.1969\n",
      "Epoch [13/100], Step [4000/4306], Loss: 2.0538\n",
      "Epoch [13/100], Mean Epoch Loss: 2.8380, Mean Validation Loss: 2.1007\n",
      "==================================================\n",
      "Epoch [14/100], Step [1000/4306], Loss: 2.8343\n",
      "Epoch [14/100], Step [2000/4306], Loss: 3.5726\n",
      "Epoch [14/100], Step [3000/4306], Loss: 4.3570\n",
      "Epoch [14/100], Step [4000/4306], Loss: 3.9484\n",
      "Epoch [14/100], Mean Epoch Loss: 2.7995, Mean Validation Loss: 1.9985\n",
      "==================================================\n",
      "Epoch [15/100], Step [1000/4306], Loss: 3.1240\n",
      "Epoch [15/100], Step [2000/4306], Loss: 1.2697\n",
      "Epoch [15/100], Step [3000/4306], Loss: 4.6036\n",
      "Epoch [15/100], Step [4000/4306], Loss: 4.8548\n",
      "Epoch [15/100], Mean Epoch Loss: 2.7705, Mean Validation Loss: 2.1360\n",
      "==================================================\n",
      "Epoch [16/100], Step [1000/4306], Loss: 1.8234\n",
      "Epoch [16/100], Step [2000/4306], Loss: 1.7784\n",
      "Epoch [16/100], Step [3000/4306], Loss: 2.5927\n",
      "Epoch [16/100], Step [4000/4306], Loss: 2.5170\n",
      "Epoch [16/100], Mean Epoch Loss: 2.7518, Mean Validation Loss: 2.0099\n",
      "==================================================\n",
      "Epoch [17/100], Step [1000/4306], Loss: 1.8111\n",
      "Epoch [17/100], Step [2000/4306], Loss: 3.2740\n",
      "Epoch [17/100], Step [3000/4306], Loss: 2.8932\n",
      "Epoch [17/100], Step [4000/4306], Loss: 2.0841\n",
      "Epoch [17/100], Mean Epoch Loss: 2.7365, Mean Validation Loss: 1.9635\n",
      "==================================================\n",
      "Epoch [18/100], Step [1000/4306], Loss: 3.2724\n",
      "Epoch [18/100], Step [2000/4306], Loss: 3.3086\n",
      "Epoch [18/100], Step [3000/4306], Loss: 1.8424\n",
      "Epoch [18/100], Step [4000/4306], Loss: 1.1512\n",
      "Epoch [18/100], Mean Epoch Loss: 2.7188, Mean Validation Loss: 1.7917\n",
      "==================================================\n",
      "Epoch [19/100], Step [1000/4306], Loss: 1.9718\n",
      "Epoch [19/100], Step [2000/4306], Loss: 1.5638\n",
      "Epoch [19/100], Step [3000/4306], Loss: 4.1534\n",
      "Epoch [19/100], Step [4000/4306], Loss: 1.5369\n",
      "Epoch [19/100], Mean Epoch Loss: 2.7060, Mean Validation Loss: 1.8650\n",
      "==================================================\n",
      "Epoch [20/100], Step [1000/4306], Loss: 3.8523\n",
      "Epoch [20/100], Step [2000/4306], Loss: 1.8835\n",
      "Epoch [20/100], Step [3000/4306], Loss: 2.5192\n",
      "Epoch [20/100], Step [4000/4306], Loss: 2.1978\n",
      "Epoch [20/100], Mean Epoch Loss: 2.6930, Mean Validation Loss: 2.1515\n",
      "==================================================\n",
      "Model saved at epoch 20: ./checkpoints/wldnn_window200_20eps_seed2023.pth\n",
      "Epoch [21/100], Step [1000/4306], Loss: 1.6764\n",
      "Epoch [21/100], Step [2000/4306], Loss: 3.6598\n",
      "Epoch [21/100], Step [3000/4306], Loss: 2.2382\n",
      "Epoch [21/100], Step [4000/4306], Loss: 2.4594\n",
      "Epoch [21/100], Mean Epoch Loss: 2.6796, Mean Validation Loss: 1.7868\n",
      "==================================================\n",
      "Epoch [22/100], Step [1000/4306], Loss: 2.1829\n",
      "Epoch [22/100], Step [2000/4306], Loss: 2.1695\n",
      "Epoch [22/100], Step [3000/4306], Loss: 3.0602\n",
      "Epoch [22/100], Step [4000/4306], Loss: 2.8726\n",
      "Epoch [22/100], Mean Epoch Loss: 2.6718, Mean Validation Loss: 1.8744\n",
      "==================================================\n",
      "Epoch [23/100], Step [1000/4306], Loss: 2.2551\n",
      "Epoch [23/100], Step [2000/4306], Loss: 1.5987\n",
      "Epoch [23/100], Step [3000/4306], Loss: 3.8980\n",
      "Epoch [23/100], Step [4000/4306], Loss: 4.7056\n",
      "Epoch [23/100], Mean Epoch Loss: 2.6585, Mean Validation Loss: 1.8791\n",
      "==================================================\n",
      "Epoch [24/100], Step [1000/4306], Loss: 3.2760\n",
      "Epoch [24/100], Step [2000/4306], Loss: 1.8452\n",
      "Epoch [24/100], Step [3000/4306], Loss: 2.4142\n",
      "Epoch [24/100], Step [4000/4306], Loss: 1.6197\n",
      "Epoch [24/100], Mean Epoch Loss: 2.6522, Mean Validation Loss: 2.0646\n",
      "==================================================\n",
      "Epoch [25/100], Step [1000/4306], Loss: 4.1119\n",
      "Epoch [25/100], Step [2000/4306], Loss: 2.8750\n",
      "Epoch [25/100], Step [3000/4306], Loss: 1.9114\n",
      "Epoch [25/100], Step [4000/4306], Loss: 2.2813\n",
      "Epoch [25/100], Mean Epoch Loss: 2.6391, Mean Validation Loss: 1.8803\n",
      "==================================================\n",
      "Epoch [26/100], Step [1000/4306], Loss: 3.6651\n",
      "Epoch [26/100], Step [2000/4306], Loss: 1.5149\n",
      "Epoch [26/100], Step [3000/4306], Loss: 2.0836\n",
      "Epoch [26/100], Step [4000/4306], Loss: 2.1895\n",
      "Epoch [26/100], Mean Epoch Loss: 2.6301, Mean Validation Loss: 2.1290\n",
      "==================================================\n",
      "Epoch [27/100], Step [1000/4306], Loss: 2.8862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Step [2000/4306], Loss: 1.7433\n",
      "Epoch [27/100], Step [3000/4306], Loss: 3.1240\n",
      "Epoch [27/100], Step [4000/4306], Loss: 1.9466\n",
      "Epoch [27/100], Mean Epoch Loss: 2.6236, Mean Validation Loss: 2.1168\n",
      "==================================================\n",
      "Epoch [28/100], Step [1000/4306], Loss: 3.3463\n",
      "Epoch [28/100], Step [2000/4306], Loss: 3.2562\n",
      "Epoch [28/100], Step [3000/4306], Loss: 2.4466\n",
      "Epoch [28/100], Step [4000/4306], Loss: 2.2001\n",
      "Epoch [28/100], Mean Epoch Loss: 2.6135, Mean Validation Loss: 1.9067\n",
      "==================================================\n",
      "Epoch [29/100], Step [1000/4306], Loss: 2.3397\n",
      "Epoch [29/100], Step [2000/4306], Loss: 2.6693\n",
      "Epoch [29/100], Step [3000/4306], Loss: 2.4804\n",
      "Epoch [29/100], Step [4000/4306], Loss: 2.5065\n",
      "Epoch [29/100], Mean Epoch Loss: 2.6054, Mean Validation Loss: 1.8339\n",
      "==================================================\n",
      "Epoch [30/100], Step [1000/4306], Loss: 2.5953\n",
      "Epoch [30/100], Step [2000/4306], Loss: 2.9441\n",
      "Epoch [30/100], Step [3000/4306], Loss: 2.3881\n",
      "Epoch [30/100], Step [4000/4306], Loss: 2.4600\n",
      "Epoch [30/100], Mean Epoch Loss: 2.5997, Mean Validation Loss: 2.0146\n",
      "==================================================\n",
      "Model saved at epoch 30: ./checkpoints/wldnn_window200_30eps_seed2023.pth\n",
      "Epoch [31/100], Step [1000/4306], Loss: 5.6380\n",
      "Epoch [31/100], Step [2000/4306], Loss: 2.4954\n",
      "Epoch [31/100], Step [3000/4306], Loss: 2.3724\n",
      "Epoch [31/100], Step [4000/4306], Loss: 2.3554\n",
      "Epoch [31/100], Mean Epoch Loss: 2.5911, Mean Validation Loss: 1.8334\n",
      "==================================================\n",
      "Epoch [32/100], Step [1000/4306], Loss: 2.4548\n",
      "Epoch [32/100], Step [2000/4306], Loss: 3.2872\n",
      "Epoch [32/100], Step [3000/4306], Loss: 2.1636\n",
      "Epoch [32/100], Step [4000/4306], Loss: 1.6363\n",
      "Epoch [32/100], Mean Epoch Loss: 2.5870, Mean Validation Loss: 1.7353\n",
      "==================================================\n",
      "Epoch [33/100], Step [1000/4306], Loss: 1.6682\n",
      "Epoch [33/100], Step [2000/4306], Loss: 4.2544\n",
      "Epoch [33/100], Step [3000/4306], Loss: 3.3825\n",
      "Epoch [33/100], Step [4000/4306], Loss: 2.4106\n",
      "Epoch [33/100], Mean Epoch Loss: 2.5774, Mean Validation Loss: 1.8572\n",
      "==================================================\n",
      "Epoch [34/100], Step [1000/4306], Loss: 1.4343\n",
      "Epoch [34/100], Step [2000/4306], Loss: 2.4048\n",
      "Epoch [34/100], Step [3000/4306], Loss: 1.7116\n",
      "Epoch [34/100], Step [4000/4306], Loss: 2.2602\n",
      "Epoch [34/100], Mean Epoch Loss: 2.5724, Mean Validation Loss: 2.2390\n",
      "==================================================\n",
      "Epoch [35/100], Step [1000/4306], Loss: 2.5590\n",
      "Epoch [35/100], Step [2000/4306], Loss: 1.1885\n",
      "Epoch [35/100], Step [3000/4306], Loss: 2.2096\n",
      "Epoch [35/100], Step [4000/4306], Loss: 2.9925\n",
      "Epoch [35/100], Mean Epoch Loss: 2.5659, Mean Validation Loss: 1.8670\n",
      "==================================================\n",
      "Epoch [36/100], Step [1000/4306], Loss: 1.4770\n",
      "Epoch [36/100], Step [2000/4306], Loss: 3.7511\n",
      "Epoch [36/100], Step [3000/4306], Loss: 2.3568\n",
      "Epoch [36/100], Step [4000/4306], Loss: 4.2078\n",
      "Epoch [36/100], Mean Epoch Loss: 2.5585, Mean Validation Loss: 2.1843\n",
      "==================================================\n",
      "Epoch [37/100], Step [1000/4306], Loss: 5.6079\n",
      "Epoch [37/100], Step [2000/4306], Loss: 2.3906\n",
      "Epoch [37/100], Step [3000/4306], Loss: 2.8068\n",
      "Epoch [37/100], Step [4000/4306], Loss: 2.8842\n",
      "Epoch [37/100], Mean Epoch Loss: 2.5543, Mean Validation Loss: 1.8286\n",
      "==================================================\n",
      "Epoch [38/100], Step [1000/4306], Loss: 2.8929\n",
      "Epoch [38/100], Step [2000/4306], Loss: 3.3392\n",
      "Epoch [38/100], Step [3000/4306], Loss: 2.5948\n",
      "Epoch [38/100], Step [4000/4306], Loss: 1.2567\n",
      "Epoch [38/100], Mean Epoch Loss: 2.5483, Mean Validation Loss: 2.0142\n",
      "==================================================\n",
      "Epoch [39/100], Step [1000/4306], Loss: 2.8964\n",
      "Epoch [39/100], Step [2000/4306], Loss: 2.4130\n",
      "Epoch [39/100], Step [3000/4306], Loss: 2.0267\n",
      "Epoch [39/100], Step [4000/4306], Loss: 2.4076\n",
      "Epoch [39/100], Mean Epoch Loss: 2.5416, Mean Validation Loss: 1.7565\n",
      "==================================================\n",
      "Epoch [40/100], Step [1000/4306], Loss: 2.0343\n",
      "Epoch [40/100], Step [2000/4306], Loss: 4.3861\n",
      "Epoch [40/100], Step [3000/4306], Loss: 1.8678\n",
      "Epoch [40/100], Step [4000/4306], Loss: 1.5452\n",
      "Epoch [40/100], Mean Epoch Loss: 2.5392, Mean Validation Loss: 1.9072\n",
      "==================================================\n",
      "Model saved at epoch 40: ./checkpoints/wldnn_window200_40eps_seed2023.pth\n",
      "Epoch [41/100], Step [1000/4306], Loss: 3.5640\n",
      "Epoch [41/100], Step [2000/4306], Loss: 1.2921\n",
      "Epoch [41/100], Step [3000/4306], Loss: 2.7316\n",
      "Epoch [41/100], Step [4000/4306], Loss: 2.6534\n",
      "Epoch [41/100], Mean Epoch Loss: 2.5349, Mean Validation Loss: 1.8664\n",
      "==================================================\n",
      "Epoch [42/100], Step [1000/4306], Loss: 1.8994\n",
      "Epoch [42/100], Step [2000/4306], Loss: 4.6352\n",
      "Epoch [42/100], Step [3000/4306], Loss: 2.0747\n",
      "Epoch [42/100], Step [4000/4306], Loss: 2.6099\n",
      "Epoch [42/100], Mean Epoch Loss: 2.5289, Mean Validation Loss: 1.7958\n",
      "==================================================\n",
      "Epoch [43/100], Step [1000/4306], Loss: 2.2667\n",
      "Epoch [43/100], Step [2000/4306], Loss: 2.2280\n",
      "Epoch [43/100], Step [3000/4306], Loss: 2.6103\n",
      "Epoch [43/100], Step [4000/4306], Loss: 2.4074\n",
      "Epoch [43/100], Mean Epoch Loss: 2.5270, Mean Validation Loss: 2.1781\n",
      "==================================================\n",
      "Epoch [44/100], Step [1000/4306], Loss: 1.4695\n",
      "Epoch [44/100], Step [2000/4306], Loss: 2.7572\n",
      "Epoch [44/100], Step [3000/4306], Loss: 1.7753\n",
      "Epoch [44/100], Step [4000/4306], Loss: 4.3348\n",
      "Epoch [44/100], Mean Epoch Loss: 2.5227, Mean Validation Loss: 1.7857\n",
      "==================================================\n",
      "Epoch [45/100], Step [1000/4306], Loss: 1.8267\n",
      "Epoch [45/100], Step [2000/4306], Loss: 1.5853\n",
      "Epoch [45/100], Step [3000/4306], Loss: 1.8585\n",
      "Epoch [45/100], Step [4000/4306], Loss: 4.4109\n",
      "Epoch [45/100], Mean Epoch Loss: 2.5189, Mean Validation Loss: 1.8776\n",
      "==================================================\n",
      "Epoch [46/100], Step [1000/4306], Loss: 2.0515\n",
      "Epoch [46/100], Step [2000/4306], Loss: 1.8352\n",
      "Epoch [46/100], Step [3000/4306], Loss: 3.9646\n",
      "Epoch [46/100], Step [4000/4306], Loss: 2.3620\n",
      "Epoch [46/100], Mean Epoch Loss: 2.5152, Mean Validation Loss: 1.8230\n",
      "==================================================\n",
      "Epoch [47/100], Step [1000/4306], Loss: 2.1928\n",
      "Epoch [47/100], Step [2000/4306], Loss: 2.4138\n",
      "Epoch [47/100], Step [3000/4306], Loss: 2.2759\n",
      "Epoch [47/100], Step [4000/4306], Loss: 2.4779\n",
      "Epoch [47/100], Mean Epoch Loss: 2.5114, Mean Validation Loss: 1.8347\n",
      "==================================================\n",
      "Epoch [48/100], Step [1000/4306], Loss: 2.5242\n",
      "Epoch [48/100], Step [2000/4306], Loss: 1.6639\n",
      "Epoch [48/100], Step [3000/4306], Loss: 4.0805\n",
      "Epoch [48/100], Step [4000/4306], Loss: 2.8714\n",
      "Epoch [48/100], Mean Epoch Loss: 2.5051, Mean Validation Loss: 1.9769\n",
      "==================================================\n",
      "Epoch [49/100], Step [1000/4306], Loss: 1.4347\n",
      "Epoch [49/100], Step [2000/4306], Loss: 2.9280\n",
      "Epoch [49/100], Step [3000/4306], Loss: 3.3998\n",
      "Epoch [49/100], Step [4000/4306], Loss: 3.0007\n",
      "Epoch [49/100], Mean Epoch Loss: 2.5064, Mean Validation Loss: 1.8883\n",
      "==================================================\n",
      "Epoch [50/100], Step [1000/4306], Loss: 2.7503\n",
      "Epoch [50/100], Step [2000/4306], Loss: 2.9657\n",
      "Epoch [50/100], Step [3000/4306], Loss: 1.5061\n",
      "Epoch [50/100], Step [4000/4306], Loss: 1.7743\n",
      "Epoch [50/100], Mean Epoch Loss: 2.5029, Mean Validation Loss: 1.9459\n",
      "==================================================\n",
      "Model saved at epoch 50: ./checkpoints/wldnn_window200_50eps_seed2023.pth\n",
      "Epoch [51/100], Step [1000/4306], Loss: 2.2730\n",
      "Epoch [51/100], Step [2000/4306], Loss: 3.1279\n",
      "Epoch [51/100], Step [3000/4306], Loss: 2.3554\n",
      "Epoch [51/100], Step [4000/4306], Loss: 4.2229\n",
      "Epoch [51/100], Mean Epoch Loss: 2.5005, Mean Validation Loss: 1.8782\n",
      "==================================================\n",
      "Epoch [52/100], Step [1000/4306], Loss: 1.8516\n",
      "Epoch [52/100], Step [2000/4306], Loss: 3.2800\n",
      "Epoch [52/100], Step [3000/4306], Loss: 2.4816\n",
      "Epoch [52/100], Step [4000/4306], Loss: 2.1606\n",
      "Epoch [52/100], Mean Epoch Loss: 2.4948, Mean Validation Loss: 1.9484\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [1000/4306], Loss: 1.5234\n",
      "Epoch [53/100], Step [2000/4306], Loss: 3.4914\n",
      "Epoch [53/100], Step [3000/4306], Loss: 2.5504\n",
      "Epoch [53/100], Step [4000/4306], Loss: 1.9714\n",
      "Epoch [53/100], Mean Epoch Loss: 2.4906, Mean Validation Loss: 1.8653\n",
      "==================================================\n",
      "Epoch [54/100], Step [1000/4306], Loss: 3.5359\n",
      "Epoch [54/100], Step [2000/4306], Loss: 1.9585\n",
      "Epoch [54/100], Step [3000/4306], Loss: 3.0480\n",
      "Epoch [54/100], Step [4000/4306], Loss: 1.6142\n",
      "Epoch [54/100], Mean Epoch Loss: 2.4907, Mean Validation Loss: 1.7494\n",
      "==================================================\n",
      "Epoch [55/100], Step [1000/4306], Loss: 2.1549\n",
      "Epoch [55/100], Step [2000/4306], Loss: 1.7289\n",
      "Epoch [55/100], Step [3000/4306], Loss: 1.8454\n",
      "Epoch [55/100], Step [4000/4306], Loss: 5.0534\n",
      "Epoch [55/100], Mean Epoch Loss: 2.4874, Mean Validation Loss: 2.0478\n",
      "==================================================\n",
      "Epoch [56/100], Step [1000/4306], Loss: 1.2757\n",
      "Epoch [56/100], Step [2000/4306], Loss: 4.7137\n",
      "Epoch [56/100], Step [3000/4306], Loss: 2.8795\n",
      "Epoch [56/100], Step [4000/4306], Loss: 2.0831\n",
      "Epoch [56/100], Mean Epoch Loss: 2.4850, Mean Validation Loss: 2.0011\n",
      "==================================================\n",
      "Epoch [57/100], Step [1000/4306], Loss: 1.4422\n",
      "Epoch [57/100], Step [2000/4306], Loss: 2.8487\n",
      "Epoch [57/100], Step [3000/4306], Loss: 2.1585\n",
      "Epoch [57/100], Step [4000/4306], Loss: 1.7064\n",
      "Epoch [57/100], Mean Epoch Loss: 2.4832, Mean Validation Loss: 1.9452\n",
      "==================================================\n",
      "Epoch [58/100], Step [1000/4306], Loss: 2.2521\n",
      "Epoch [58/100], Step [2000/4306], Loss: 5.3766\n",
      "Epoch [58/100], Step [3000/4306], Loss: 2.2172\n",
      "Epoch [58/100], Step [4000/4306], Loss: 2.4140\n",
      "Epoch [58/100], Mean Epoch Loss: 2.4779, Mean Validation Loss: 1.9796\n",
      "==================================================\n",
      "Epoch [59/100], Step [1000/4306], Loss: 2.9663\n",
      "Epoch [59/100], Step [2000/4306], Loss: 2.8961\n",
      "Epoch [59/100], Step [3000/4306], Loss: 1.7152\n",
      "Epoch [59/100], Step [4000/4306], Loss: 2.7400\n",
      "Epoch [59/100], Mean Epoch Loss: 2.4758, Mean Validation Loss: 2.0400\n",
      "==================================================\n",
      "Epoch [60/100], Step [1000/4306], Loss: 2.1208\n",
      "Epoch [60/100], Step [2000/4306], Loss: 2.9769\n",
      "Epoch [60/100], Step [3000/4306], Loss: 1.7497\n",
      "Epoch [60/100], Step [4000/4306], Loss: 1.8957\n",
      "Epoch [60/100], Mean Epoch Loss: 2.4750, Mean Validation Loss: 1.7681\n",
      "==================================================\n",
      "Model saved at epoch 60: ./checkpoints/wldnn_window200_60eps_seed2023.pth\n",
      "Epoch [61/100], Step [1000/4306], Loss: 2.8125\n",
      "Epoch [61/100], Step [2000/4306], Loss: 1.9428\n",
      "Epoch [61/100], Step [3000/4306], Loss: 3.3218\n",
      "Epoch [61/100], Step [4000/4306], Loss: 2.9593\n",
      "Epoch [61/100], Mean Epoch Loss: 2.4745, Mean Validation Loss: 1.9633\n",
      "==================================================\n",
      "Epoch [62/100], Step [1000/4306], Loss: 1.8028\n",
      "Epoch [62/100], Step [2000/4306], Loss: 1.5048\n",
      "Epoch [62/100], Step [3000/4306], Loss: 3.8548\n",
      "Epoch [62/100], Step [4000/4306], Loss: 2.3096\n",
      "Epoch [62/100], Mean Epoch Loss: 2.4687, Mean Validation Loss: 1.7878\n",
      "==================================================\n",
      "Epoch [63/100], Step [1000/4306], Loss: 2.5286\n",
      "Epoch [63/100], Step [2000/4306], Loss: 1.6719\n",
      "Epoch [63/100], Step [3000/4306], Loss: 5.4540\n",
      "Epoch [63/100], Step [4000/4306], Loss: 1.6138\n",
      "Epoch [63/100], Mean Epoch Loss: 2.4678, Mean Validation Loss: 2.0300\n",
      "==================================================\n",
      "Epoch [64/100], Step [1000/4306], Loss: 1.7502\n",
      "Epoch [64/100], Step [2000/4306], Loss: 3.8664\n",
      "Epoch [64/100], Step [3000/4306], Loss: 2.8962\n",
      "Epoch [64/100], Step [4000/4306], Loss: 1.9351\n",
      "Epoch [64/100], Mean Epoch Loss: 2.4644, Mean Validation Loss: 1.8653\n",
      "==================================================\n",
      "Epoch [65/100], Step [1000/4306], Loss: 1.5116\n",
      "Epoch [65/100], Step [2000/4306], Loss: 2.0927\n",
      "Epoch [65/100], Step [3000/4306], Loss: 1.7320\n",
      "Epoch [65/100], Step [4000/4306], Loss: 1.8937\n",
      "Epoch [65/100], Mean Epoch Loss: 2.4656, Mean Validation Loss: 2.0207\n",
      "==================================================\n",
      "Epoch [66/100], Step [1000/4306], Loss: 2.4127\n",
      "Epoch [66/100], Step [2000/4306], Loss: 0.6365\n",
      "Epoch [66/100], Step [3000/4306], Loss: 2.0548\n",
      "Epoch [66/100], Step [4000/4306], Loss: 1.0746\n",
      "Epoch [66/100], Mean Epoch Loss: 2.4638, Mean Validation Loss: 1.8803\n",
      "==================================================\n",
      "Epoch [67/100], Step [1000/4306], Loss: 2.2553\n",
      "Epoch [67/100], Step [2000/4306], Loss: 3.1189\n",
      "Epoch [67/100], Step [3000/4306], Loss: 2.0548\n",
      "Epoch [67/100], Step [4000/4306], Loss: 2.8992\n",
      "Epoch [67/100], Mean Epoch Loss: 2.4608, Mean Validation Loss: 1.7898\n",
      "==================================================\n",
      "Epoch [68/100], Step [1000/4306], Loss: 1.4372\n",
      "Epoch [68/100], Step [2000/4306], Loss: 2.2103\n",
      "Epoch [68/100], Step [3000/4306], Loss: 2.8643\n",
      "Epoch [68/100], Step [4000/4306], Loss: 2.5268\n",
      "Epoch [68/100], Mean Epoch Loss: 2.4593, Mean Validation Loss: 1.7411\n",
      "==================================================\n",
      "Epoch [69/100], Step [1000/4306], Loss: 2.7204\n",
      "Epoch [69/100], Step [2000/4306], Loss: 2.5957\n",
      "Epoch [69/100], Step [3000/4306], Loss: 2.9414\n",
      "Epoch [69/100], Step [4000/4306], Loss: 3.9587\n",
      "Epoch [69/100], Mean Epoch Loss: 2.4579, Mean Validation Loss: 2.1465\n",
      "==================================================\n",
      "Epoch [70/100], Step [1000/4306], Loss: 2.7473\n",
      "Epoch [70/100], Step [2000/4306], Loss: 2.1840\n",
      "Epoch [70/100], Step [3000/4306], Loss: 1.9269\n",
      "Epoch [70/100], Step [4000/4306], Loss: 1.8237\n",
      "Epoch [70/100], Mean Epoch Loss: 2.4557, Mean Validation Loss: 1.9677\n",
      "==================================================\n",
      "Model saved at epoch 70: ./checkpoints/wldnn_window200_70eps_seed2023.pth\n",
      "Epoch [71/100], Step [1000/4306], Loss: 1.5723\n",
      "Epoch [71/100], Step [2000/4306], Loss: 2.9627\n",
      "Epoch [71/100], Step [3000/4306], Loss: 1.7043\n",
      "Epoch [71/100], Step [4000/4306], Loss: 1.6455\n",
      "Epoch [71/100], Mean Epoch Loss: 2.4513, Mean Validation Loss: 1.7457\n",
      "==================================================\n",
      "Epoch [72/100], Step [1000/4306], Loss: 4.5410\n",
      "Epoch [72/100], Step [2000/4306], Loss: 2.2044\n",
      "Epoch [72/100], Step [3000/4306], Loss: 2.5050\n",
      "Epoch [72/100], Step [4000/4306], Loss: 1.7827\n",
      "Epoch [72/100], Mean Epoch Loss: 2.4530, Mean Validation Loss: 1.9317\n",
      "==================================================\n",
      "Epoch [73/100], Step [1000/4306], Loss: 2.8469\n",
      "Epoch [73/100], Step [2000/4306], Loss: 3.1599\n",
      "Epoch [73/100], Step [3000/4306], Loss: 1.8593\n",
      "Epoch [73/100], Step [4000/4306], Loss: 1.7118\n",
      "Epoch [73/100], Mean Epoch Loss: 2.4515, Mean Validation Loss: 1.9168\n",
      "==================================================\n",
      "Epoch [74/100], Step [1000/4306], Loss: 0.9724\n",
      "Epoch [74/100], Step [2000/4306], Loss: 1.8477\n",
      "Epoch [74/100], Step [3000/4306], Loss: 2.2176\n",
      "Epoch [74/100], Step [4000/4306], Loss: 2.2982\n",
      "Epoch [74/100], Mean Epoch Loss: 2.4510, Mean Validation Loss: 1.9049\n",
      "==================================================\n",
      "Epoch [75/100], Step [1000/4306], Loss: 1.6365\n",
      "Epoch [75/100], Step [2000/4306], Loss: 1.7765\n",
      "Epoch [75/100], Step [3000/4306], Loss: 2.7794\n",
      "Epoch [75/100], Step [4000/4306], Loss: 1.8064\n",
      "Epoch [75/100], Mean Epoch Loss: 2.4437, Mean Validation Loss: 1.9792\n",
      "==================================================\n",
      "Epoch [76/100], Step [1000/4306], Loss: 2.4533\n",
      "Epoch [76/100], Step [2000/4306], Loss: 4.5735\n",
      "Epoch [76/100], Step [3000/4306], Loss: 1.5475\n",
      "Epoch [76/100], Step [4000/4306], Loss: 3.5882\n",
      "Epoch [76/100], Mean Epoch Loss: 2.4475, Mean Validation Loss: 1.8508\n",
      "==================================================\n",
      "Epoch [77/100], Step [1000/4306], Loss: 1.7512\n",
      "Epoch [77/100], Step [2000/4306], Loss: 1.6039\n",
      "Epoch [77/100], Step [3000/4306], Loss: 3.2992\n",
      "Epoch [77/100], Step [4000/4306], Loss: 2.2198\n",
      "Epoch [77/100], Mean Epoch Loss: 2.4437, Mean Validation Loss: 1.8013\n",
      "==================================================\n",
      "Epoch [78/100], Step [1000/4306], Loss: 4.9738\n",
      "Epoch [78/100], Step [2000/4306], Loss: 2.0333\n",
      "Epoch [78/100], Step [3000/4306], Loss: 1.6022\n",
      "Epoch [78/100], Step [4000/4306], Loss: 1.9606\n",
      "Epoch [78/100], Mean Epoch Loss: 2.4432, Mean Validation Loss: 2.0185\n",
      "==================================================\n",
      "Epoch [79/100], Step [1000/4306], Loss: 1.7173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Step [2000/4306], Loss: 3.3088\n",
      "Epoch [79/100], Step [3000/4306], Loss: 1.7855\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "save_every = 10  # Save the model every 10 epochs\n",
    "steps_per_print = 1000  # Print loss every 1000 steps\n",
    "\n",
    "# Initialize a list to store the losses\n",
    "train_losses = []\n",
    "val_losses = []  # Initialize a list to store validation losses\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    step_counter = 0  # Initialize step counter for the epoch\n",
    "    epoch_loss = 0.0  # Initialize the epoch loss\n",
    "\n",
    "    for batch_data, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_data)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate the loss for the epoch\n",
    "\n",
    "        # Increment step counter\n",
    "        step_counter += 1\n",
    "\n",
    "        # Print loss every `steps_per_print` steps\n",
    "        if step_counter % steps_per_print == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step_counter}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and record the mean loss for the epoch\n",
    "    mean_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(mean_epoch_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data, val_labels in val_dataloader:\n",
    "            val_predictions = model(val_data)\n",
    "            val_loss += criterion(val_predictions, val_labels).item()\n",
    "\n",
    "    mean_val_loss = val_loss / len(val_dataloader)\n",
    "    val_losses.append(mean_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Epoch Loss: {mean_epoch_loss:.4f}, Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "    print(\"==================================================\")\n",
    "\n",
    "    # Save the model every `save_every` epochs\n",
    "    if (epoch + 1) % save_every == 0:\n",
    "        model_checkpoint_path = f'./checkpoints/{model_name.lower()}_window{WINDOW_SIZE}_{epoch+1}eps_seed{seed}.pth'\n",
    "        torch.save(model.state_dict(), model_checkpoint_path)\n",
    "        print(f\"Model saved at epoch {epoch+1}: {model_checkpoint_path}\")\n",
    "\n",
    "print(\"Training completed\")\n",
    "torch.save(model.state_dict(), f'./checkpoints/{model_name.lower()}_window{WINDOW_SIZE}_{num_epochs}eps_seed{seed}.pth')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eba0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training time in seconds\n",
    "training_time_seconds = end - start\n",
    "# Convert training time to minutes\n",
    "training_time_minutes = training_time_seconds // 60\n",
    "\n",
    "# Print the training time in minutes\n",
    "print(f\"Training time: {training_time_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()  # Show legend indicating the lines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242186f",
   "metadata": {},
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54999789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and labels\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "# Iterate through validation data\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in val_dataloader:\n",
    "        predictions = model(batch_data)\n",
    "        val_predictions.append(predictions.cpu())\n",
    "        val_labels.append(batch_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and labels\n",
    "val_predictions_np = np.concatenate(val_predictions, axis=0)\n",
    "val_labels_np = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "# Reshape predictions and labels\n",
    "val_predictions_flat = val_predictions_np.reshape(-1)\n",
    "val_labels_flat = val_labels_np.reshape(-1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(val_labels_flat, val_predictions_flat)\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "correlation_matrix = np.corrcoef(val_labels_flat, val_predictions_flat)\n",
    "correlation_coefficient = correlation_matrix[0, 1]\n",
    "spearmanr_cc, _ = stats.spearmanr(val_labels_flat, val_predictions_flat)\n",
    "\n",
    "# Plot predictions against true labels\n",
    "plt.scatter(val_labels_flat, val_predictions_flat, alpha=0.5, s=10, color='none', edgecolors='black')\n",
    "plt.xlabel('True SNR')\n",
    "plt.ylabel('Estimated SNR')\n",
    "plt.title(f'CC = {correlation_coefficient:.4f}')\n",
    "\n",
    "# Set x-axis and y-axis limits\n",
    "plt.xlim(-17.5, 2.5)\n",
    "plt.ylim(-17.5, 2.5)\n",
    "\n",
    "# Perform linear regression\n",
    "m, b = np.polyfit(val_labels_flat, val_predictions_flat, 1)\n",
    "plt.plot(val_labels_flat, m * val_labels_flat + b, label=f\"y = {m:.2f}x + {b:.2f}\")\n",
    "plt.plot(val_labels_flat, val_labels_flat, label=\"y = x\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(f'Correlation coefficient (CC): {correlation_coefficient:.4f}')\n",
    "print(f'Spearman rank-order correlation coefficient (SRCC): {spearmanr_cc:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the square error for each element\n",
    "error = (val_labels_flat - val_predictions_flat)**2\n",
    "\n",
    "# Calculate mean and standard deviation of the error\n",
    "mean_error = np.mean(error)\n",
    "std_deviation_error = np.std(error)\n",
    "\n",
    "# Plot the error distribution as a histogram\n",
    "hist, bins, _ = plt.hist(error, bins=10, alpha=0.7, color='blue', edgecolor='black', range=(0, 5))\n",
    "plt.xlabel('Squared Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Squared Error Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate each bar with the frequency\n",
    "for i in range(len(hist)):\n",
    "    plt.text(bins[i] + (bins[i+1] - bins[i]) / 2, hist[i], f'{int(hist[i])}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Error:\", mean_error)\n",
    "print(\"Standard Deviation of Error:\", std_deviation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb37046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute error for each element\n",
    "error = np.abs(val_labels_flat - val_predictions_flat)\n",
    "\n",
    "# Calculate mean and standard deviation of the error\n",
    "mean_error = np.mean(error)\n",
    "std_deviation_error = np.std(error)\n",
    "\n",
    "# Plot the error distribution as a histogram\n",
    "hist, bins, _ = plt.hist(error, bins=10, alpha=0.7, color='blue', edgecolor='black', range=(0, 5))\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Squared Error')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate each bar with the frequency\n",
    "for i in range(len(hist)):\n",
    "    plt.text(bins[i] + (bins[i+1] - bins[i]) / 2, hist[i], f'{int(hist[i])}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mean_error)\n",
    "print(\"Standard Deviation of Absolute Error:\", std_deviation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868411fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snr",
   "language": "python",
   "name": "snr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
